# é¢„çƒ­é˜¶æ®µç­–ç•¥ï¼šæ·±åº¦åˆ†æä¸å®æ–½æ–¹æ¡ˆ

## ğŸ¯ æ ¸å¿ƒæ´å¯Ÿï¼šä½ çš„é‡‡é›†å‡½æ•°å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆ

é€šè¿‡åˆ†æä½ çš„ `EURAnovaPairAcqf` ä»£ç ï¼Œæˆ‘å‘ç°äº†å…³é”®æœºåˆ¶ï¼š

### 1. å‚æ•°æ–¹å·®ç‡ r_t æ˜¯æ ¸å¿ƒæ§åˆ¶å˜é‡
```python
r_t = Var[Î¸|D_current] / Var[Î¸|D_initial]  # ç¬¬952è¡Œ
```
- **r_t â‰ˆ 1.0**ï¼šå‚æ•°æœªæ”¶æ•›ï¼ŒÎ»_t â†’ Î»_min (0.1)ï¼Œ**ä¸»æ•ˆåº”ä¼˜å…ˆ**
- **r_t < 0.3**ï¼šå‚æ•°å·²æ”¶æ•›ï¼ŒÎ»_t â†’ Î»_max (1.0)ï¼Œ**äº¤äº’æ•ˆåº”ä¼˜å…ˆ**

### 2. åŠ¨æ€æƒé‡æœºåˆ¶çš„å«ä¹‰
ä½ çš„é‡‡é›†å‡½æ•°é€šè¿‡ Î»_t è‡ªåŠ¨è°ƒæ•´ä¸»æ•ˆåº”ä¸äº¤äº’æ•ˆåº”çš„æ¢ç´¢æƒé‡ï¼š
- **æ—©æœŸï¼ˆr_té«˜ï¼‰**ï¼šä¸“æ³¨ä¸»æ•ˆåº”ï¼Œå»ºç«‹ç¨³å®šåŸºçº¿
- **åæœŸï¼ˆr_tä½ï¼‰**ï¼šè½¬å‘äº¤äº’æ•ˆåº”ï¼Œç²¾ç»†åŒ–æ¢ç´¢

**å…³é”®å¯ç¤º**ï¼šé¢„çƒ­é˜¶æ®µåº”è¯¥è®© r_t å¿«é€Ÿä¸‹é™ï¼Œè¿™æ ·ä¸»åŠ¨å­¦ä¹ é˜¶æ®µæ‰èƒ½æœ‰æ•ˆæ¢ç´¢äº¤äº’ï¼

---

## ğŸ“Š é¢„çƒ­ç­–ç•¥æ¨èï¼š**ç­–ç•¥A - çº¯Space-filling**

### ä¸ºä»€ä¹ˆé€‰æ‹©Space-fillingè€Œéå…¶ä»–ç­–ç•¥ï¼Ÿ

| ç­–ç•¥ | ä¸»æ•ˆåº”ä¼°è®¡ | r_tä¸‹é™é€Ÿåº¦ | äº¤äº’ç­›é€‰è´¨é‡ | æ¨èåº¦ |
|------|-----------|------------|-------------|---------|
| **A: Space-filling (LHS)** | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | **æœ€ä¼˜** |
| B: D-optimal | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | â˜…â˜… | æ¬¡é€‰ |
| C: éšæœºé‡‡æ · | â˜…â˜… | â˜…â˜… | â˜…â˜… | ä¸æ¨è |

### é€‰æ‹©LHSçš„ç†è®ºä¾æ®ï¼š

1. **å‡åŒ€è¦†ç›– â†’ ä¸»æ•ˆåº”å‡†ç¡®ä¼°è®¡**
   - LHSä¿è¯æ¯ä¸ªå› å­çš„æ¯ä¸ªæ°´å¹³éƒ½è¢«å……åˆ†é‡‡æ ·
   - ä¸»æ•ˆåº”æ–¹å·®å¿«é€Ÿä¸‹é™ â†’ r_tå¿«é€Ÿä¸‹é™

2. **r_tå¿«é€Ÿä¸‹é™ â†’ Î»_tè‡ªåŠ¨ä¸Šå‡**
   - æ— éœ€æ‰‹åŠ¨è°ƒæ•´ï¼Œä½ çš„åŠ¨æ€æœºåˆ¶ä¼šè‡ªåŠ¨å·¥ä½œ
   - é¢„çƒ­ç»“æŸæ—¶ï¼ŒÎ»_tä¼šè‡ªç„¶å‡é«˜ï¼Œä¸ºäº¤äº’æ¢ç´¢åšå¥½å‡†å¤‡

3. **æ— åäº¤äº’ç­›é€‰**
   - Space-fillingé¿å…äº†é‡‡æ ·åå·®
   - äº¤äº’æ•ˆåº”çš„på€¼æ›´å¯é 

---

## ğŸ”§ é¢„çƒ­é˜¶æ®µå®æ–½æ–¹æ¡ˆ

### Step 1: ç”ŸæˆLHSé‡‡æ ·ç‚¹ï¼ˆ160ä¸ªï¼‰

```python
from pyDOE2 import lhs
from scipy.spatial.distance import pdist
import numpy as np

def generate_maximin_lhs(n_samples=160, n_factors=6, n_iterations=100):
    """ç”ŸæˆMaximin LHSè®¾è®¡"""
    best_design = None
    best_min_dist = -1
    
    for _ in range(n_iterations):
        # ç”ŸæˆLHS
        design = lhs(n_factors, samples=n_samples, criterion='maximin')
        
        # è®¡ç®—æœ€å°è·ç¦»
        min_dist = pdist(design).min()
        
        if min_dist > best_min_dist:
            best_min_dist = min_dist
            best_design = design
    
    return best_design

# ç”Ÿæˆé¢„çƒ­è®¾è®¡
X_warmup = generate_maximin_lhs(160, 6)
```

### Step 2: æ”¶é›†æ•°æ®ï¼ˆ8äººÃ—20æ¬¡ï¼‰

```python
# åˆ†é…ç»™8ä¸ªè¢«è¯•
subject_assignments = np.repeat(np.arange(8), 20)
np.random.shuffle(subject_assignments)  # éšæœºåˆ†é…

for i, subject_id in enumerate(subject_assignments):
    x_trial = X_warmup[i]
    y_response = collect_response(subject_id, x_trial)
    save_data(subject_id, x_trial, y_response)
```

### Step 3: åˆ†æé¢„çƒ­æ•°æ® - **å…³é”®æ­¥éª¤**

```python
import statsmodels.formula.api as smf
import pandas as pd

def analyze_warmup_data(X_warmup, y_warmup, subject_ids):
    """
    åˆ†æé¢„çƒ­æ•°æ®ï¼Œæå–å…³é”®ä¿¡æ¯ç”¨äºä¸»åŠ¨å­¦ä¹ é…ç½®
    
    Returns:
        dict: åŒ…å«ç­›é€‰çš„äº¤äº’å¯¹ã€å‚æ•°ä¼°è®¡ã€è¶…å‚æ•°å»ºè®®
    """
    
    # 1. æ„å»ºæ•°æ®æ¡†
    df = pd.DataFrame(X_warmup, columns=[f'x{i}' for i in range(6)])
    df['y'] = y_warmup
    df['subject'] = subject_ids
    
    # 2. æ‹Ÿåˆæ··åˆæ•ˆåº”æ¨¡å‹ï¼ˆä¸»æ•ˆåº” + æ‰€æœ‰äºŒé˜¶äº¤äº’ï¼‰
    formula = 'y ~ x0 + x1 + x2 + x3 + x4 + x5'  # ä¸»æ•ˆåº”
    
    # æ·»åŠ æ‰€æœ‰å¯èƒ½çš„äº¤äº’é¡¹
    interactions = []
    for i in range(6):
        for j in range(i+1, 6):
            interactions.append(f'x{i}:x{j}')
    
    formula += ' + ' + ' + '.join(interactions)
    formula += ' + (1|subject)'  # éšæœºæˆªè·
    
    model = smf.mixedlm(formula, df, groups=df["subject"])
    result = model.fit()
    
    # 3. ç­›é€‰æ˜¾è‘—äº¤äº’ï¼ˆp < 0.10ï¼‰
    significant_pairs = []
    for i in range(6):
        for j in range(i+1, 6):
            coef_name = f'x{i}:x{j}'
            if coef_name in result.params:
                p_value = result.pvalues[coef_name]
                if p_value < 0.10:  # å®½æ¾é˜ˆå€¼
                    significant_pairs.append((i, j))
                    print(f"äº¤äº’ ({i},{j}): Î²={result.params[coef_name]:.3f}, p={p_value:.3f}")
    
    # 4. è®¡ç®—ä¸»æ•ˆåº”æ–¹å·®ï¼ˆç”¨äºä¼°è®¡r_tï¼‰
    main_effects_var = {}
    for i in range(6):
        coef_name = f'x{i}'
        if coef_name in result.params:
            # ä½¿ç”¨æ ‡å‡†è¯¯çš„å¹³æ–¹ä½œä¸ºæ–¹å·®ä¼°è®¡
            main_effects_var[i] = result.bse[coef_name] ** 2
    
    # 5. ä¼°è®¡åˆå§‹r_tï¼ˆä¸ºä¸»åŠ¨å­¦ä¹ æä¾›åŸºçº¿ï¼‰
    avg_var = np.mean(list(main_effects_var.values()))
    
    # 6. è¶…å‚æ•°å»ºè®®
    hyperparams = {
        'lambda_max': 1.0 if len(significant_pairs) > 3 else 0.7,  # äº¤äº’å¤šåˆ™æƒé‡é«˜
        'tau_n_max': 21,  # 70%ä½ç½®è½¬å‘ç²¾ç»†åŒ–
        'tau1': 0.7,  # r_tä¸Šé˜ˆå€¼
        'tau2': 0.3,  # r_tä¸‹é˜ˆå€¼
        'initial_variance_estimate': avg_var
    }
    
    return {
        'interaction_pairs': significant_pairs,
        'main_effects': {f'x{i}': result.params[f'x{i}'] for i in range(6)},
        'main_effects_se': {f'x{i}': result.bse[f'x{i}'] for i in range(6)},
        'hyperparams': hyperparams,
        'model_summary': result.summary()
    }
```

### Step 4: é…ç½®ä¸»åŠ¨å­¦ä¹ é˜¶æ®µ

```python
# ä½¿ç”¨é¢„çƒ­åˆ†æç»“æœé…ç½®é‡‡é›†å‡½æ•°
warmup_results = analyze_warmup_data(X_warmup, y_warmup, subject_ids)

# ä¸ºæ¯ä¸ªåç»­è¢«è¯•é…ç½®é‡‡é›†å‡½æ•°
def create_acquisition_function(model, warmup_results):
    """åŸºäºé¢„çƒ­ç»“æœåˆ›å»ºä¼˜åŒ–çš„é‡‡é›†å‡½æ•°"""
    
    return EURAnovaPairAcqf(
        model=model,
        # ä½¿ç”¨ç­›é€‰çš„äº¤äº’å¯¹
        interaction_pairs=warmup_results['interaction_pairs'],
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è¶…å‚æ•°
        lambda_max=warmup_results['hyperparams']['lambda_max'],
        lambda_min=0.1,
        tau1=warmup_results['hyperparams']['tau1'],
        tau2=warmup_results['hyperparams']['tau2'],
        tau_n_max=warmup_results['hyperparams']['tau_n_max'],
        
        # åŠ¨æ€æƒé‡æœºåˆ¶
        use_dynamic_lambda=True,
        use_dynamic_gamma=True,
        
        # å…¶ä»–å‚æ•°
        gamma=0.3,
        gamma_min=0.05,
        total_budget=30,  # æ¯ä¸ªè¢«è¯•30æ¬¡
    )
```

---

## ğŸ“ˆ é¢„çƒ­äº§å‡ºæ¸…å•

### å¿…é¡»è·å¾—çš„ç»Ÿè®¡é‡ï¼š

1. **äº¤äº’å¯¹ç­›é€‰åˆ—è¡¨**
   - 5-7ä¸ª p<0.10 çš„äº¤äº’å¯¹
   - æ ¼å¼ï¼š`[(0,1), (2,5), (3,4), ...]`

2. **ä¸»æ•ˆåº”ä¼°è®¡**
   - æ¯ä¸ªå› å­çš„ç³»æ•°å’Œæ ‡å‡†è¯¯
   - ç”¨äºéªŒè¯æ¨¡å‹æ”¶æ•›æ€§

3. **å‚æ•°æ–¹å·®åŸºçº¿**
   - åˆå§‹æ–¹å·®ä¼°è®¡ï¼Œç”¨äºr_tè®¡ç®—
   - åˆ¤æ–­ä½•æ—¶ä»æ¢ç´¢è½¬å‘ç²¾ç»†åŒ–

4. **è¶…å‚æ•°é…ç½®**
   - `lambda_max`ï¼šåŸºäºäº¤äº’å¯¹æ•°é‡
   - `tau_n_max`ï¼šè½¬å‘ç‚¹è®¾ç½®

### å¯é€‰ä½†æœ‰ä»·å€¼çš„åˆ†æï¼š

1. **ä¸ªä½“å·®å¼‚è¯„ä¼°**
   ```python
   # éšæœºæ•ˆåº”æ–¹å·®
   random_var = result.cov_re
   ICC = random_var / (random_var + residual_var)
   print(f"ç»„å†…ç›¸å…³ç³»æ•°ICC: {ICC:.3f}")
   ```

2. **æ•ˆåº”é‡æ’åº**
   ```python
   # æ ‡å‡†åŒ–æ•ˆåº”é‡
   effect_sizes = {}
   for param, value in result.params.items():
       if ':' not in param and param != 'Intercept':
           effect_sizes[param] = abs(value) / result.bse[param]
   
   sorted_effects = sorted(effect_sizes.items(), key=lambda x: x[1], reverse=True)
   ```

3. **è®¾è®¡è¯Šæ–­**
   ```python
   # VIFæ£€æŸ¥ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰
   from statsmodels.stats.outliers_influence import variance_inflation_factor
   
   vif = [variance_inflation_factor(X_warmup, i) for i in range(6)]
   print(f"VIFå€¼: {vif}")  # åº”è¯¥éƒ½<5
   ```

---

## âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹

### 1. ä¸è¦è¿‡åº¦ç­›é€‰äº¤äº’
- ä½¿ç”¨ p<0.10 è€Œé p<0.05
- å®å¯åŒ…å«å‡é˜³æ€§ï¼Œä¸è¦é”™è¿‡çœŸå®æ•ˆåº”
- ä½ çš„åŠ¨æ€Î»_tä¼šè‡ªåŠ¨é™ä½ä¸é‡è¦äº¤äº’çš„æƒé‡

### 2. ä¿æŒé¢„çƒ­çš„çº¯ç²¹æ€§
- **ä¸è¦**åœ¨é¢„çƒ­é˜¶æ®µä½¿ç”¨ä¸»åŠ¨å­¦ä¹ 
- **ä¸è¦**æ ¹æ®ä¸­æœŸç»“æœè°ƒæ•´è®¾è®¡
- åšæŒå®Œæˆå…¨éƒ¨160ä¸ªLHSç‚¹

### 3. æ­£ç¡®ä½¿ç”¨é¢„çƒ­ç»“æœ
- é¢„çƒ­æ˜¯ä¸ºäº†**é…ç½®**ä¸»åŠ¨å­¦ä¹ ï¼Œä¸æ˜¯ä¸ºäº†**å¾—å‡ºç»“è®º**
- æ•ˆåº”ä¼°è®¡æ˜¯åˆæ­¥çš„ï¼Œæœ€ç»ˆåˆ†æä½¿ç”¨å…¨éƒ¨æ•°æ®

---

## ğŸ’¡ æœ€ç»ˆå»ºè®®

ä½ çš„é‡‡é›†å‡½æ•°è®¾è®¡ç²¾å¦™ï¼ŒåŠ¨æ€æœºåˆ¶ä¼šè‡ªåŠ¨é€‚åº”æ•°æ®ã€‚é¢„çƒ­é˜¶æ®µçš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼š

1. **è®©r_tå¿«é€Ÿä¸‹é™**ï¼ˆé€šè¿‡LHSå‡åŒ€é‡‡æ ·ï¼‰
2. **ç­›é€‰å€™é€‰äº¤äº’å¯¹**ï¼ˆé¿å…æµªè´¹æ¢ç´¢ï¼‰
3. **æä¾›è¶…å‚æ•°åŸºçº¿**ï¼ˆä¼˜åŒ–åç»­è¡¨ç°ï¼‰

è®°ä½ï¼š**LHSé¢„çƒ­ â†’ r_tå¿«é€Ÿä¸‹é™ â†’ Î»_tè‡ªåŠ¨ä¸Šå‡ â†’ äº¤äº’æ¢ç´¢è‡ªç„¶å¼€å¯**

è¿™æ˜¯ä¸€ä¸ªè‡ªæ´½çš„ç³»ç»Ÿï¼Œé¢„çƒ­åšå¥½äº†ï¼Œåé¢çš„ä¸»åŠ¨å­¦ä¹ ä¼šè‡ªåŠ¨é«˜æ•ˆè¿è¡Œï¼
