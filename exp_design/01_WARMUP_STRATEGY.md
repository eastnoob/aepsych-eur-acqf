# é¢„çƒ­é˜¶æ®µæ–¹æ¡ˆè¯¦è§£

## ğŸ¯ é¢„çƒ­é˜¶æ®µçš„ç›®æ ‡

**ä¸æ˜¯**ï¼šè®­ç»ƒ"æœ€ç»ˆæ¨¡å‹"ï¼ˆä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼‰
**è€Œæ˜¯**ï¼šMeta-learningï¼ˆå­¦ä¹ å¦‚ä½•åšå®éªŒï¼‰

### ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡

1. **ç­›é€‰äº¤äº’å¯¹**ï¼šä»15ä¸ªå¯èƒ½çš„äº¤äº’å¯¹ä¸­ï¼Œæ‰¾å‡º"æœ‰å¸Œæœ›"çš„5-7ä¸ª
2. **ç¡®å®šè¶…å‚æ•°**ï¼šä¸ºåç»­20ä¸ªè¢«è¯•çš„é‡‡é›†å‡½æ•°æ‰¾åˆ°æœ€ä¼˜é…ç½®
3. **éªŒè¯æœºåˆ¶**ï¼šæ£€æŸ¥åŠ¨æ€æƒé‡ï¼ˆÎ»_tã€Î³_tï¼‰æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ

---

## ğŸ“Š é¢„çƒ­é…ç½®

### åŸºæœ¬å‚æ•°

```python
é¢„çƒ­é˜¶æ®µ = {
    "è¢«è¯•æ•°": 8,
    "æ¯äººé‡‡æ ·": 20æ¬¡,
    "æ€»æ ·æœ¬": 160,
    "é‡‡æ ·æ–¹æ³•": "Maximin Latin Hypercube Sampling",
    "æ˜¯å¦ä¸»åŠ¨å­¦ä¹ ": False  # çº¯Space-filling
}
```

### ä¸ºä»€ä¹ˆæ˜¯8äººÃ—20æ¬¡ï¼Ÿ

**è¢«è¯•æ•°é€‰æ‹©ï¼ˆ8äººï¼‰ï¼š**
```
å¤ªå°‘ï¼ˆ<5äººï¼‰ï¼š
  - ç»Ÿè®¡åŠŸæ•ˆä¸è¶³ï¼ˆäº¤äº’å¯¹ç­›é€‰ä¸å¯é ï¼‰
  - ä¸ªä½“å·®å¼‚æœªå……åˆ†ä½“ç°
  - è¶…å‚æ•°ä¼°è®¡å¯èƒ½åç¦»

åˆé€‚ï¼ˆ8-10äººï¼‰ï¼š
  - æ¯ä¸ªäº¤äº’å¯¹çº¦10.7ä¸ªæ ·æœ¬ï¼ˆ160Ã·15=10.7ï¼‰
  - æ··åˆæ¨¡å‹å¯ä»¥ç»™å‡ºåˆæ­¥ä¼°è®¡
  - è¶…å‚æ•°ä¸­ä½æ•°è¾ƒé²æ£’

å¤ªå¤šï¼ˆ>12äººï¼‰ï¼š
  - æŒ¤å ä¸»åŠ¨å­¦ä¹ é¢„ç®—ï¼ˆæœºä¼šæˆæœ¬é«˜ï¼‰
  - é¢„çƒ­ç›®çš„æ˜¯"ç­›é€‰"ä¸æ˜¯"ç²¾ç¡®ä¼°è®¡"
```

**æ¯äººé‡‡æ ·æ•°ï¼ˆ20æ¬¡ï¼‰ï¼š**
```
å¤ªå°‘ï¼ˆ<15æ¬¡ï¼‰ï¼š
  - GPæ¨¡å‹éš¾ä»¥å­¦ä¹ 6ç»´ç©ºé—´
  - r_tä¸‹é™ä¸æ˜æ˜¾ï¼ˆæ— æ³•éªŒè¯åŠ¨æ€æœºåˆ¶ï¼‰

åˆé€‚ï¼ˆ20æ¬¡ï¼‰ï¼š
  - è¶³å¤Ÿè¦†ç›–ä¸»æ•ˆåº”çš„ä¸»è¦æ°´å¹³
  - LHSå¯ä»¥ç”Ÿæˆè‰¯å¥½çš„Space-fillingåˆ†å¸ƒ

å¤ªå¤šï¼ˆ>25æ¬¡ï¼‰ï¼š
  - æµªè´¹é¢„ç®—ï¼ˆä¸»åŠ¨å­¦ä¹ æ›´é«˜æ•ˆï¼‰
  - é¢„çƒ­åªéœ€"åˆæ­¥è¦†ç›–"
```

---

## ğŸ”§ ç­–ç•¥Aï¼šMaximin LHSï¼ˆæ¨èï¼‰

### è®¾è®¡åŸç†

**Latin Hypercube Sampling (LHS)**ï¼š
- å°†æ¯ä¸ªç»´åº¦åˆ†æˆnå±‚
- æ¯å±‚æ°å¥½é‡‡æ ·ä¸€æ¬¡
- ä¿è¯è¾¹é™…åˆ†å¸ƒå‡åŒ€

**Maximinä¼˜åŒ–**ï¼š
- æœ€å¤§åŒ–æ ·æœ¬é—´çš„æœ€å°è·ç¦»
- é¿å…æ ·æœ¬èšé›†
- æé«˜ç©ºé—´å¡«å……è´¨é‡

### ç”Ÿæˆä»£ç 

```python
from scipy.stats import qmc
import numpy as np

def generate_warmup_design(n_factors=6, n_samples=20, seed=42):
    """
    ç”ŸæˆMaximin LHSé¢„çƒ­è®¾è®¡

    Args:
        n_factors: å› å­æ•°é‡
        n_samples: æ¯ä¸ªè¢«è¯•çš„æ ·æœ¬æ•°
        seed: éšæœºç§å­ï¼ˆç¡®ä¿å¯é‡å¤ï¼‰

    Returns:
        X_warmup: (n_samples, n_factors) çš„æ ‡å‡†åŒ–æ ·æœ¬ [0,1]
    """
    # åˆ›å»ºLHSé‡‡æ ·å™¨
    sampler = qmc.LatinHypercube(d=n_factors, optimization="random-cd", seed=seed)

    # ç”Ÿæˆæ ‡å‡†åŒ–æ ·æœ¬ [0, 1]
    X_warmup = sampler.random(n=n_samples)

    # è®¡ç®—è¦†ç›–è´¨é‡
    from scipy.spatial.distance import pdist
    min_dist = pdist(X_warmup).min()
    print(f"âœ“ æœ€å°æˆå¯¹è·ç¦»: {min_dist:.4f}")

    return X_warmup

# ä¸ºæ¯ä¸ªè¢«è¯•ç”Ÿæˆç‹¬ç«‹çš„LHSè®¾è®¡
for subject_id in range(8):
    X_warmup = generate_warmup_design(n_factors=6, n_samples=20, seed=42+subject_id)

    # æ˜ å°„åˆ°å®é™…å› å­èŒƒå›´
    # X_scaled = map_to_factor_ranges(X_warmup, factor_configs)
```

### æ˜ å°„åˆ°å®é™…å› å­èŒƒå›´

```python
def map_to_factor_ranges(X_normalized, factor_configs):
    """
    å°†æ ‡å‡†åŒ–æ ·æœ¬æ˜ å°„åˆ°å®é™…å› å­èŒƒå›´

    Args:
        X_normalized: (n, d) æ ‡å‡†åŒ–æ ·æœ¬ [0,1]
        factor_configs: å› å­é…ç½®åˆ—è¡¨
            ä¾‹å¦‚ï¼š[
                {"type": "continuous", "range": [0, 10]},
                {"type": "categorical", "levels": [0, 1, 2]},
                {"type": "integer", "range": [1, 20]}
            ]

    Returns:
        X_mapped: (n, d) æ˜ å°„åçš„æ ·æœ¬
    """
    n, d = X_normalized.shape
    X_mapped = np.zeros((n, d))

    for i, config in enumerate(factor_configs):
        if config["type"] == "continuous":
            # è¿ç»­å˜é‡ï¼šçº¿æ€§æ˜ å°„
            low, high = config["range"]
            X_mapped[:, i] = X_normalized[:, i] * (high - low) + low

        elif config["type"] == "categorical":
            # åˆ†ç±»å˜é‡ï¼šåˆ†å±‚é‡‡æ ·
            levels = config["levels"]
            n_levels = len(levels)
            indices = (X_normalized[:, i] * n_levels).astype(int)
            indices = np.clip(indices, 0, n_levels - 1)
            X_mapped[:, i] = [levels[idx] for idx in indices]

        elif config["type"] == "integer":
            # æ•´æ•°å˜é‡ï¼šèˆå…¥
            low, high = config["range"]
            X_mapped[:, i] = np.round(X_normalized[:, i] * (high - low) + low)

    return X_mapped
```

---

## ğŸ§ª é¢„çƒ­é˜¶æ®µçš„æ•°æ®æ”¶é›†

### æµç¨‹å›¾

```
å¯¹äºæ¯ä¸ªè¢«è¯• s âˆˆ {0, 1, ..., 7}:
    1. åˆå§‹åŒ–ç‹¬ç«‹GPæ¨¡å‹ model_s
    2. ç”ŸæˆLHSè®¾è®¡ X_warmup_s (20ä¸ªæ ·æœ¬)
    3. å¯¹äºæ¯ä¸ªæ ·æœ¬ x âˆˆ X_warmup_s:
        a. è¿è¡Œå®éªŒ y = run_trial(subject_id=s, x=x)
        b. æ›´æ–°æ¨¡å‹ model_s.update(x, y)
        c. è®°å½•æ•°æ® (x, y, s, trial_number)
    4. ä¿å­˜è¢«è¯•æ•°æ®
```

### æ•°æ®è®°å½•æ ¼å¼

```python
warmup_data = []

for subject_id in range(8):
    model_individual = GPModel()
    X_warmup = generate_warmup_design(n_factors=6, n_samples=20, seed=42+subject_id)
    X_scaled = map_to_factor_ranges(X_warmup, factor_configs)

    for trial, x in enumerate(X_scaled):
        # è¿è¡Œå®éªŒ
        y = run_trial(subject_id, x)

        # æ›´æ–°æ¨¡å‹
        model_individual.update(x, y)

        # è®°å½•æ•°æ®
        warmup_data.append({
            "subject_id": subject_id,
            "trial": trial,
            "phase": "warmup",
            "x0": x[0],
            "x1": x[1],
            "x2": x[2],
            "x3": x[3],
            "x4": x[4],
            "x5": x[5],
            "y": y,
            "timestamp": datetime.now()
        })

# è½¬æ¢ä¸ºDataFrame
import pandas as pd
df_warmup = pd.DataFrame(warmup_data)
df_warmup.to_csv("warmup_data.csv", index=False)
```

---

## ğŸ“ˆ Meta-learningåˆ†æ

### æ­¥éª¤1ï¼šæ‹Ÿåˆåˆæ­¥æ··åˆæ¨¡å‹

```python
import statsmodels.formula.api as smf

# æ„é€ åŒ…å«æ‰€æœ‰äº¤äº’çš„å…¬å¼
formula = "y ~ x0 + x1 + x2 + x3 + x4 + x5"  # ä¸»æ•ˆåº”

# æ·»åŠ æ‰€æœ‰15ä¸ªäºŒé˜¶äº¤äº’
for i in range(6):
    for j in range(i+1, 6):
        formula += f" + x{i}:x{j}"

# æ·»åŠ éšæœºæ•ˆåº”ï¼ˆéšæœºæˆªè·ï¼‰
formula += " + (1 | subject_id)"

# æ‹Ÿåˆæ¨¡å‹
preliminary_model = smf.mixedlm(formula, data=df_warmup, groups=df_warmup["subject_id"])
fitted_model = preliminary_model.fit(method="lbfgs")

print(fitted_model.summary())
```

### æ­¥éª¤2ï¼šç­›é€‰äº¤äº’å¯¹

```python
# æå–äº¤äº’é¡¹çš„på€¼
interaction_results = []

for i in range(6):
    for j in range(i+1, 6):
        param_name = f"x{i}:x{j}"
        if param_name in fitted_model.pvalues.index:
            p_value = fitted_model.pvalues[param_name]
            coef = fitted_model.fe_params[param_name]
            interaction_results.append({
                "pair": (i, j),
                "coef": coef,
                "p_value": p_value,
                "abs_coef": abs(coef)
            })

# æ’åºå¹¶ç­›é€‰
df_interactions = pd.DataFrame(interaction_results)
df_interactions = df_interactions.sort_values("abs_coef", ascending=False)

# ç­–ç•¥1ï¼šä½¿ç”¨på€¼é˜ˆå€¼ï¼ˆå®½æ¾ï¼‰
significant_pairs_p = [
    row["pair"] for _, row in df_interactions.iterrows()
    if row["p_value"] < 0.10  # å®½æ¾é˜ˆå€¼
]

# ç­–ç•¥2ï¼šä½¿ç”¨Top-Kï¼ˆæ›´ä¿å®ˆï¼‰
significant_pairs_topk = [
    row["pair"] for _, row in df_interactions.head(5).iterrows()
]

# æ¨èï¼šç»“åˆä¸¤ç§ç­–ç•¥
# å¦‚æœp<0.10çš„äº¤äº’å¯¹<3ä¸ªï¼Œä½¿ç”¨Top-5
# å¦‚æœp<0.10çš„äº¤äº’å¯¹>8ä¸ªï¼Œåªä¿ç•™Top-8
if len(significant_pairs_p) < 3:
    selected_pairs = significant_pairs_topk[:5]
elif len(significant_pairs_p) > 8:
    selected_pairs = [
        row["pair"] for _, row in df_interactions.iterrows()
        if row["p_value"] < 0.10
    ][:8]
else:
    selected_pairs = significant_pairs_p

print(f"âœ“ é€‰æ‹©çš„äº¤äº’å¯¹: {selected_pairs}")
```

### æ­¥éª¤3ï¼šç¡®å®šé‡‡é›†å‡½æ•°è¶…å‚æ•°

```python
# 3.1 åˆ†ææ¯ä¸ªè¢«è¯•çš„r_tè½¨è¿¹ï¼ˆå¦‚æœè®°å½•äº†ï¼‰
# è¿™éœ€è¦åœ¨é¢„çƒ­è¿‡ç¨‹ä¸­è·Ÿè¸ªå‚æ•°æ–¹å·®
# ç•¥è¿‡ï¼ˆéœ€è¦ä¿®æ”¹æ•°æ®æ”¶é›†ä»£ç ï¼‰

# 3.2 ç¡®å®šlambda_max
# æ–¹æ³•ï¼šåŸºäºäº¤äº’æ•ˆåº”çš„ç›¸å¯¹é‡è¦æ€§
main_effects_var = fitted_model.fe_params[["x0", "x1", "x2", "x3", "x4", "x5"]].var()
interaction_effects_var = df_interactions["abs_coef"].var()

if interaction_effects_var > main_effects_var * 0.5:
    recommended_lambda_max = 1.0  # äº¤äº’å¾ˆé‡è¦
elif interaction_effects_var > main_effects_var * 0.2:
    recommended_lambda_max = 0.8  # äº¤äº’ä¸­ç­‰é‡è¦
else:
    recommended_lambda_max = 0.5  # äº¤äº’ä¸å¤ªé‡è¦

print(f"âœ“ æ¨èlambda_max: {recommended_lambda_max}")

# 3.3 ç¡®å®štau_n_max
# è§„åˆ™ï¼šä¸»åŠ¨å­¦ä¹ é¢„ç®—çš„70%ä½ç½®
n_active_trials = 30  # æ¯ä¸ªè¢«è¯•30æ¬¡é‡‡æ ·
recommended_tau_n_max = int(n_active_trials * 0.7)
print(f"âœ“ æ¨ètau_n_max: {recommended_tau_n_max}")

# 3.4 ç¡®å®šgamma_min
# è§„åˆ™ï¼šå°æ ·æœ¬é¢„ç®—ä½¿ç”¨è¾ƒå°çš„gamma_min
if n_active_trials < 30:
    recommended_gamma_min = 0.05
else:
    recommended_gamma_min = 0.1
print(f"âœ“ æ¨ègamma_min: {recommended_gamma_min}")
```

---

## âœ… é¢„çƒ­é˜¶æ®µæ£€æŸ¥æ¸…å•

### æ•°æ®æ”¶é›†å‰

- [ ] ç¡®è®¤å› å­ç±»å‹å’ŒèŒƒå›´ï¼ˆè¿ç»­/åˆ†ç±»/æ•´æ•°ï¼‰
- [ ] ç”Ÿæˆå¹¶éªŒè¯LHSè®¾è®¡ï¼ˆæ£€æŸ¥æœ€å°è·ç¦»ï¼‰
- [ ] æµ‹è¯•GPæ¨¡å‹é…ç½®ï¼ˆæ ¸å‡½æ•°ã€ä¼¼ç„¶å‡½æ•°ï¼‰
- [ ] å‡†å¤‡æ•°æ®è®°å½•ä»£ç ï¼ˆCSVæˆ–æ•°æ®åº“ï¼‰

### æ•°æ®æ”¶é›†ä¸­

- [ ] æ¯ä¸ªè¢«è¯•ç‹¬ç«‹å»ºæ¨¡ï¼ˆä¸å…±äº«æ•°æ®ï¼‰
- [ ] å®æ—¶æ›´æ–°GPæ¨¡å‹ï¼ˆæ¯æ¬¡é‡‡æ ·åupdateï¼‰
- [ ] è®°å½•å®Œæ•´å…ƒæ•°æ®ï¼ˆè¢«è¯•IDã€è½®æ¬¡ã€æ—¶é—´æˆ³ï¼‰
- [ ] å¤‡ä»½æ•°æ®ï¼ˆé˜²æ­¢ä¸¢å¤±ï¼‰

### æ•°æ®æ”¶é›†å

- [ ] æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼ˆ160ä¸ªæ ·æœ¬ï¼Œ8ä¸ªè¢«è¯•ï¼‰
- [ ] æ‹Ÿåˆåˆæ­¥æ··åˆæ¨¡å‹ï¼ˆæ£€æŸ¥æ”¶æ•›æ€§ï¼‰
- [ ] ç­›é€‰äº¤äº’å¯¹ï¼ˆä½¿ç”¨p<0.10æˆ–Top-Kï¼‰
- [ ] ç¡®å®šè¶…å‚æ•°ï¼ˆlambda_max, tau_n_max, gamma_minï¼‰
- [ ] ç”ŸæˆMeta-learningæŠ¥å‘Š

---

## ğŸ“Š é¢„æœŸç»“æœ

### æ­£å¸¸æƒ…å†µ

```
âœ“ 8ä¸ªè¢«è¯•å®Œæˆæ•°æ®æ”¶é›†ï¼ˆ160æ ·æœ¬ï¼‰
âœ“ æ··åˆæ¨¡å‹æ”¶æ•›ï¼ˆAIC/BICåˆç†ï¼‰
âœ“ ç­›é€‰å‡º5-7ä¸ªäº¤äº’å¯¹ï¼ˆp<0.10ï¼‰
âœ“ ä¸»æ•ˆåº”ç³»æ•°æ˜¾è‘—ï¼ˆè‡³å°‘4ä¸ªp<0.05ï¼‰
âœ“ éšæœºæ•ˆåº”æ–¹å·®åˆç†ï¼ˆä¸ä¸º0ï¼‰
```

### å¼‚å¸¸æƒ…å†µå¤„ç†

**æƒ…å†µ1ï¼šæ²¡æœ‰æ˜¾è‘—äº¤äº’ï¼ˆæ‰€æœ‰p>0.10ï¼‰**
```
åŸå› ï¼š
  - çœŸå®æƒ…å†µä¸‹å¯èƒ½ç¡®å®æ— äº¤äº’
  - æ ·æœ¬é‡ä¸è¶³ï¼ˆç»Ÿè®¡åŠŸæ•ˆä½ï¼‰

å¤„ç†ï¼š
  - ä½¿ç”¨Top-5äº¤äº’å¯¹ï¼ˆåŸºäºç³»æ•°å¤§å°ï¼‰
  - åœ¨ä¸»åŠ¨å­¦ä¹ ä¸­ä»é…ç½®è¿™äº›äº¤äº’å¯¹
  - æœ€ç»ˆåˆ†æä¼šç»™å‡ºå‡†ç¡®çš„på€¼
```

**æƒ…å†µ2ï¼šæ¨¡å‹ä¸æ”¶æ•›**
```
åŸå› ï¼š
  - GPæ¨¡å‹é…ç½®é—®é¢˜ï¼ˆæ ¸å‡½æ•°ã€è¶…å‚æ•°ï¼‰
  - æ•°æ®è´¨é‡é—®é¢˜ï¼ˆå¼‚å¸¸å€¼ã€ç¼ºå¤±å€¼ï¼‰

å¤„ç†ï¼š
  - æ£€æŸ¥æ•°æ®ï¼ˆå¯è§†åŒ–ã€å¼‚å¸¸å€¼æ£€æµ‹ï¼‰
  - è°ƒæ•´GPè¶…å‚æ•°ï¼ˆlengthscaleã€noiseï¼‰
  - ç®€åŒ–æ··åˆæ¨¡å‹ï¼ˆåªç”¨éšæœºæˆªè·ï¼‰
```

**æƒ…å†µ3ï¼šr_tæœªä¸‹é™**
```
åŸå› ï¼š
  - LHSè¦†ç›–ä¸è¶³ï¼ˆæ ·æœ¬å¤ªå°‘ï¼‰
  - æ¨¡å‹æœªå……åˆ†å­¦ä¹ 

å¤„ç†ï¼š
  - å¢åŠ é¢„çƒ­æ ·æœ¬æ•°ï¼ˆ20â†’25ï¼‰
  - æ£€æŸ¥GPè®­ç»ƒè¿‡ç¨‹ï¼ˆæ˜¯å¦æ­£ç¡®updateï¼‰
```

---

## ğŸ”¬ é¢„çƒ­è´¨é‡è¯„ä¼°æŒ‡æ ‡

### è¦†ç›–åº¦æŒ‡æ ‡

```python
# è®¡ç®—æœ€å°æˆå¯¹è·ç¦»ï¼ˆåº”>0.1ï¼‰
from scipy.spatial.distance import pdist
min_dist = pdist(X_warmup).min()

# è®¡ç®—ç©ºé—´è¦†ç›–ç‡ï¼ˆåŸºäºVoronoiå›¾ï¼‰
from scipy.spatial import Voronoi
vor = Voronoi(X_warmup)
# ï¼ˆå…·ä½“è®¡ç®—ç•¥ï¼‰
```

### æ¨¡å‹è´¨é‡æŒ‡æ ‡

```python
# æ··åˆæ¨¡å‹RÂ²
R2_marginal = fitted_model.rsquared_marginal  # å›ºå®šæ•ˆåº”RÂ²
R2_conditional = fitted_model.rsquared_conditional  # æ€»RÂ²

# åº”æ»¡è¶³ï¼š
# - R2_marginal > 0.3ï¼ˆå›ºå®šæ•ˆåº”æœ‰è§£é‡ŠåŠ›ï¼‰
# - R2_conditional > 0.5ï¼ˆæ€»ä½“æ‹Ÿåˆè‰¯å¥½ï¼‰
```

### äº¤äº’å¯¹å¯é æ€§

```python
# ä¸€è‡´æ€§æ£€æŸ¥ï¼šBootstrapé‡é‡‡æ ·
from sklearn.utils import resample

bootstrap_results = []
for _ in range(100):
    # é‡é‡‡æ ·è¢«è¯•
    resampled_subjects = resample(range(8), n_samples=8)
    df_resampled = df_warmup[df_warmup["subject_id"].isin(resampled_subjects)]

    # é‡æ–°æ‹Ÿåˆ
    model_boot = smf.mixedlm(formula, data=df_resampled, groups=df_resampled["subject_id"]).fit()

    # è®°å½•æ˜¾è‘—äº¤äº’å¯¹
    sig_pairs = [
        (i, j) for i in range(6) for j in range(i+1, 6)
        if model_boot.pvalues.get(f"x{i}:x{j}", 1.0) < 0.10
    ]
    bootstrap_results.append(sig_pairs)

# è®¡ç®—æ¯ä¸ªäº¤äº’å¯¹çš„é€‰ä¸­ç‡
from collections import Counter
all_pairs = [pair for result in bootstrap_results for pair in result]
pair_stability = Counter(all_pairs)

# æ¨èï¼šåªé€‰æ‹©é€‰ä¸­ç‡>60%çš„äº¤äº’å¯¹
stable_pairs = [pair for pair, count in pair_stability.items() if count/100 > 0.6]
```

---

## ğŸ“ Meta-learningæŠ¥å‘Šæ¨¡æ¿

```markdown
# é¢„çƒ­é˜¶æ®µMeta-learningæŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- å®Œæˆæ—¥æœŸï¼šYYYY-MM-DD
- è¢«è¯•æ•°ï¼š8
- æ€»æ ·æœ¬ï¼š160
- æ•°æ®æ–‡ä»¶ï¼šwarmup_data.csv

## æ··åˆæ¨¡å‹ç»“æœ
- AIC: XXXX
- BIC: XXXX
- RÂ² (marginal): 0.XX
- RÂ² (conditional): 0.XX

## ä¸»æ•ˆåº”ä¼°è®¡
| å› å­ | ç³»æ•° | på€¼ | æ˜¾è‘—æ€§ |
|------|------|-----|--------|
| x0 | 0.XX | 0.XXX | ** |
| x1 | 0.XX | 0.XXX | * |
| ... | ... | ... | ... |

## äº¤äº’æ•ˆåº”ç­›é€‰
| äº¤äº’å¯¹ | ç³»æ•° | på€¼ | é€‰ä¸­ |
|--------|------|-----|------|
| (0,1) | 0.XX | 0.XXX | âœ“ |
| (2,5) | 0.XX | 0.XXX | âœ“ |
| ... | ... | ... | ... |

**é€‰ä¸­çš„äº¤äº’å¯¹ï¼ˆ5ä¸ªï¼‰**ï¼š(0,1), (2,5), (0,3), (1,4), (3,5)

## æ¨èè¶…å‚æ•°
- lambda_max: 0.8
- tau_n_max: 21
- gamma_min: 0.05
- tau1: 0.7ï¼ˆé»˜è®¤ï¼‰
- tau2: 0.3ï¼ˆé»˜è®¤ï¼‰

## è´¨é‡æŒ‡æ ‡
- æœ€å°æˆå¯¹è·ç¦»: 0.15 âœ“
- Bootstrapç¨³å®šæ€§: å¹³å‡é€‰ä¸­ç‡ 0.68 âœ“
- æ¨¡å‹æ”¶æ•›: æ˜¯ âœ“

## å»ºè®®
è¿›å…¥ä¸»åŠ¨å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨ä»¥ä¸Šé…ç½®ã€‚
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **æ ¸å¿ƒæ€è·¯**ï¼š`00_CORE_IDEAS.md`
- **å®Œæ•´å®éªŒè®¾è®¡**ï¼š`../EXPERIMENT_DESIGN.md`
- **é¢„çƒ­ä»£ç ç¤ºä¾‹**ï¼š`../warmup_strategy_example.py`
- **ç­–ç•¥å¯¹æ¯”**ï¼š`../strategy_matrix_critique.md`
