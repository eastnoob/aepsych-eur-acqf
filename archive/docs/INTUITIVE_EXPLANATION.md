"""
两个改进方向的直观对比和应用指南
"""

# ============================================================================

# 问题背景：为什么需要这两个改进？

# ============================================================================

"""
当前采集函数的单一固定扰动：

    local_jitter_frac = 0.1  （对所有维，所有点都一样）
    sigma = 0.1 * span
    noise ~ N(0, sigma²)

问题场景：
"""

# 场景1：参数空间不同位置的"语义差异"

# ──────────────────────────────────────────

"""
假设一个实验有 3 维：
  维 0：刺激强度 (0-100，连续)
  维 1：刺激频率 (1-5，分类：低/中/高/很高/极高)
  维 2：持续时间 (0-60秒，连续)

模型的学习进度：
  初期（试验 1-5）：三个维度都不确定
  中期（试验 6-15）：维 0 已学好，维 1 仍在学，维 2 中等
  晚期（试验 16-30）：维 0 已精确，维 1 学到关键交互，维 2 也成熟

当前单一固定策略的问题：
  ✗ 维 0 在晚期：扰动 0.1*100=10，还是太大！应该 2-3 就够
  ✗ 维 1 在中期：离散采样无法利用"相似度"，频率间隔是固定的
  ✗ 维 2 在晚期：0.1*60=6秒的扰动依然是一成不变

改进方向：
  ✅ 多尺度：为维 1 提供多个采样尺度（精确 vs 粗糙）
  ✅ 学习型：维 0/2 根据学习进度自动调整扰动幅度
"""

# ============================================================================

# 直观对比：单尺度 vs 多尺度

# ============================================================================

"""
═══════════════════════════════════════════════════════════════════════════
【问题】局部最小值附近 vs 平坦区域 的"好邻域"大小完全不同！
═══════════════════════════════════════════════════════════════════════════

场景：心理物理实验中的"感知阈值"曲线

感知概率
  1.0 │                                     ╱╱╱╱╱
      │                                  ╱╱╱
      │                               ╱╱╱
      │                            ╱╱╱
      │                         ╱╱╱
  0.5 │              ────────╱╱╱  ← 陡峭的 S 曲线
      │            ╱╱
      │          ╱╱
      │        ╱╱
  0.0 │╱╱╱╱╱╱╱╱
      └──────────────────────────────────────── 刺激强度
        20   25   30   35   40   45   50

【单一尺度的采集】
  在强度 = 35（S曲线中点，陡峭）：
    - 当前设置：local_jitter_frac = 0.1 → 扰动幅度 = ±3
    - 采样点：32, 33, 34, 35, 36, 37, 38 （在陡峭区间内）
    ✓ 适合：能捕捉快速变化
    ✗ 问题：无法看到更远处的全局趋势（50 处已经饱和了）

  在强度 = 15（S曲线左侧，平缓）：
    - 扰动幅度仍然 = ±3
    - 采样点：12, 13, 14, 15, 16, 17, 18 （全在平缓区）
    ✗ 问题：相邻点间概率变化很小，浪费样本
    ✗ 无法发现"阈值其实在25"这样的远距信息

【多尺度采集】
  在强度 = 35（S曲线中点，陡峭）：
    - 尺度 1 (细粒度, 0.05*100=±2.5)：33-37 （精确捕捉陡峭区间）
    - 尺度 2 (中等, 0.15*100=±7.5)：27-42 （看到转折点周边）
    - 尺度 3 (粗粒度, 0.3*100=±30)：5-65  （全局视角）
    ✓ 效果：多尺度覆盖，既精又全

  在强度 = 15（S曲线左侧，平缓）：
    - 尺度 1：12-17 （局部，仍是平缓）
    - 尺度 2：7-22  （能到达...！看到一点变化）
    - 尺度 3：-15-45 （跨越整个低响应区到中等响应区）
    ✓ 效果：粗尺度帮助发现"远处的对比"

【数量对比】
  当前：local_num = 4
  多尺度：scales=[0.05, 0.15, 0.3], points_per_scale=2
         total = 3 * 2 = 6 个点

  ⚠️ 略多一些，但对于 local evaluation 可承受
"""

# ============================================================================

# 直观对比：固定分布 vs 学习型分布

# ============================================================================

"""
═══════════════════════════════════════════════════════════════════════════
【问题】有些维度"已经成熟"，有些维度"仍在学习"，用同一个分布不合理！
═══════════════════════════════════════════════════════════════════════════

场景：Likert 量表研究，5 维混合变量

训练进程：    试验 1-5         试验 6-15         试验 16-30
             （冷启动）       （学习中）        （精调阶段）

维 0 (强度)：
  学习速度：    极快 ←────────────────── 已完全收敛
  参数方差：    1.0 ─┐
               0.8 ─┤
               0.6 ─┤
               0.4 ─┤
               0.2 ─┤  ┌────────────────
               0.0 ─┴──┘
  
  固定扰动：  σ = 0.1 * span (始终不变)
  
  改进（学习型）：
    - 试验 1-5：   σ = 0.25 *span （大幅探索，参数不确定）
    - 试验 6-15：  σ = 0.15* span （中等扰动）
    - 试验 16-30： σ = 0.03 * span （精细微调，参数已确定）
  
  ✓ 优势：早期探索全局，晚期精细化估计

维 1 (频率)：
  学习速度：    缓慢 ←────────────────── 持续学习
  参数方差：    1.0 ──┐
               0.8 ──┤ ┌───────
               0.6 ──┤ │
               0.4 ──┤─┘
               0.2 ──┤
               0.0 ──┴──────────────
  
  特殊性：常与维 2 交互
  
  固定扰动：σ = 0.1 * span
  
  改进（学习型）：
    - 试验 1-5：   σ = 0.2 *span + 厚尾分布 + 跳跃机制
                  （参数不确定，且要探索交互）
    - 试验 6-15：  σ = 0.15* span + 中等尾部
                  （逐渐聚焦，但保留多样性）
    - 试验 16-30： σ = 0.12 * span + 标准尾部
                  （仍在学习，但可精细化）
  
  关键创新：增加了"跳跃机制"
    - 正常采样：在邻域内均匀采样
    - 偶尔跳跃（20%）：跳到更远的值，探索新的交互组合
  
  ✓ 优势：长期学习过程中保持探索，且学会利用交互信息

维 2 (持续)：
  学习速度：    中等
  参数方差：    1.0 ──┐
               0.8 ──┤ ┌──────
               0.6 ──┤ │ ┌────
               0.4 ──┤─┘ │
               0.2 ──┤    │
               0.0 ──┴────┘─────
  
  特殊性：预测误差中有尾部厚（可能有极端情况）
  
  固定扰动：σ = 0.1 * span
  
  改进（学习型）：
    - 全程：σ = f(学习进度) + 1.3x放大（因为与残差相关）
    - 含义：残差与该维相关 → 该维的预测还有改进空间 → 加大探索
  
  ✓ 优势：优先重点照顾预测效果差的维度
"""

# ============================================================================

# 应用决策树

# ============================================================================

"""
我应该用哪个改进？

┌─ 问题 1：预算有限吗？（< 30 trials）
│  ├─ YES → 优先考虑多尺度（更高的信息收益）
│  └─ NO → 更多自由度，可选学习型
│
├─ 问题 2：有多个尺度的特性吗？
│  ├─ YES （如感知阈值、S曲线、分级响应）
│  │  └─ 用多尺度！ ✓ 明确有效
│  └─ NO （如线性主效应）
│     └─ 多尺度的收益较小
│
├─ 问题 3：维度的学习速度不均衡吗？
│  ├─ YES （如有"难学"维度）
│  │  └─ 用学习型！ ✓ 能自适应
│  └─ NO （所有维度表现相近）
│     └─ 学习型的收益较小
│
└─ 问题 4：能承受额外计算吗？
   ├─ YES → 可同时启用两者（最强组合）
   └─ NO → 只启用其一

【具体推荐】

场景A：心理物理学实验（感知阈值测定）
  维度：刺激强度、频率、持续时间
  特征：有明显的 S 曲线、有交互
  推荐：多尺度 ✓✓✓ + 学习型 ✓✓
  理由：多尺度捕捉曲线特性，学习型处理交互学习不均

场景B：用户偏好研究（简单线性模型）
  维度：产品特征 A、B、C...
  特征：主要是加法主效应，相互独立
  推荐：多尺度 ✓ （可选）
  理由：多尺度的收益较小，保持简单更好

场景C：工业优化（6-8 维，复杂交互）
  维度：工艺参数混合
  特征：维度间有强交互，难以预测
  推荐：多尺度 ✓✓ + 学习型 ✓✓✓
  理由：学习型能自适应维度的复杂性，多尺度提高覆盖

场景D：初期调研（第一次实验，无先验）
  维度：待定
  特征：完全未知
  推荐：多尺度 ✓✓✓
  理由：保守、通用，无需初始统计
"""

# ============================================================================

# 配置示例

# ============================================================================

"""
【示例1】仅多尺度

from extensions.dynamic_eur_acquisition.multiscale_learned_implementation import (
    MultiScaleConfig,
    EURAnovaPairAcqfEnhanced,
)

acqf = EURAnovaPairAcqfEnhanced(
    model=gp_model,
    interaction_pairs=[(0, 1), (0, 2)],
    use_multiscale=True,
    multiscale_config=MultiScaleConfig(
        scales=[0.05, 0.15, 0.3],      # 三个尺度：细-中-粗
        points_per_scale=2              # 每个尺度 2 个点 → 总共 6 个 local points
    ),
    use_learned_perturbation=False,     # 保持简单
)

【示例2】仅学习型扰动

acqf = EURAnovaPairAcqfEnhanced(
    model=gp_model,
    interaction_pairs=[(0, 1), (0, 2)],
    use_multiscale=False,               # 保持原始单一尺度
    use_learned_perturbation=True,      # 启用自适应
    # 学习统计在 forward() 时自动更新
)

【示例3】两者结合（最强组合）

acqf = EURAnovaPairAcqfEnhanced(
    model=gp_model,
    interaction_pairs=[(0, 1), (0, 2)],
    use_multiscale=True,
    multiscale_config=MultiScaleConfig(
        scales=[0.05, 0.15],             # 减少尺度数（因为学习分布已提供多样性）
        points_per_scale=2
    ),
    use_learned_perturbation=True,
    # local_jitter_frac = 0.1（但会被学习分布覆盖）
)
"""

# ============================================================================

# 性能影响分析

# ============================================================================

"""
【多尺度的开销】

GP 后验调用数：
  原始：(d + |pairs|) = d + O(d²) 次调用
  多尺度：1 次批量调用（所有点合并）
  
性能影响：
  ✓ 批量化：避免了逐维调用的开销，实际上加速了！
  ✗ 点数增加：从 local_num=4 变为 local_num=6（+50%）
    但因为合并成 1 次调用，相比原始的 21 次调用仍快 20x

总结：多尺度几乎无额外开销（反而因合并而加速）

【学习型扰动的开销】

额外计算：

  1. update_from_training：需要计算参数方差、残差统计
     - 第一次：提取初始方差（Laplace）→ O(d × n) 计算
     - 后续：更新当前方差 → 同样开销

  2. 维度采样函数：get_perturbation_sampler(dim)
     - 每维 forward 时调用一次 → O(d) 次调用
     - 每次调用内：简单的分布选择 → O(1)

开销估算（30 trials 实验）：

- 初始化：~100ms（一次性）
- 每次 forward（20 个候选点）：+50ms
- 总额外开销：< 2%（相比 GP 后验计算的秒级）

总结：学习型扰动的开销可忽略
"""

# ============================================================================

# 常见问题解答

# ============================================================================

"""
Q1：多尺度会不会造成"点聚集"？
A：不会。多尺度在不同距离生成点，但 local_num 控制了每个尺度的点数。
   合理配置（如 scales=[0.05,0.15,0.3], points_per_scale=2）
   能保证良好的分布多样性。

Q2：学习型扰动如果初期统计不足怎么办？
A：通过"halo"机制：前 5 个 trial 使用默认参数（learning_rate=0.5），
   第 6 个 trial 开始使用学习的参数。这避免了冷启动问题。

Q3：能否同时启用多尺度和学习型？
A：完全可以！互不冲突。多尺度处理"空间尺度问题"，学习型处理
  "参数不确定性问题"，两个维度独立。

Q4：哪个改进的收益更大？
A：情景相关。

- 有明显多尺度特征（如 S 曲线）→ 多尺度 > 学习型
- 学习不均衡（某些维度难学）→ 学习型 > 多尺度
- 两者都有 → 同时用效果最好

Q5：会不会过度拟合到特定的实验数据？
A：不会。两个改进都是"统计信号驱动"，而不是"过拟合参数优化"。
   即使应用到新的实验，这两个机制仍然通用。
"""
