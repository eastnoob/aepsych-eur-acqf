# 采集函数改进实验结果报告

## 执行摘要

**实验目标**: 改进采集函数以提升主效应和交互效应估计精度

**改进措施**:

1. 重复采样惩罚 (penalty_repeat=0.01)
2. 强化空间覆盖 (gamma_diversity=0.8→0.2动态降低)
3. 分区均匀性管理 (gamma_binning=0.4)
4. 信息增益优先 (动态调整交互效应权重)
5. Exploration奖励 (beta_ucb=0.15)

**实验结果**: ❌ **改进失败，性能反而下降**

## 对比结果

| 指标 | V1 (原始) | V2 (改进) | 变化 |
|------|-----------|-----------|------|
| 唯一设计数 | 39/360 (10.8%) | 28/360 (7.8%) | **-28.2%** ❌ |
| 平均重复次数 | 2.05 | 2.86 | +39.5% ❌ |
| 平均True Score | 8.72 ± 0.89 | 7.94 ± 0.60 | -0.78分 ❌ |
| 高分发现(≥9.5) | 8个 | 4个 | **-50.0%** ❌ |
| 最高分 | 10.31 | 10.19 | -0.12 |

## 失败原因分析

### 1. 重复采样问题严重恶化

**观察**: Trial 60-80出现大量重复

- `orange | grid | Font=12 | none` 至少重复10次
- `orange | grid | Font=13 | none` 重复4次  
- `orange | grid | Font=14 | none` 重复3次

**根本原因**:

- ✗ 重复惩罚力度不足 (0.01太小)
- ✗ 候选集中大量包含已采样设计
- ✗ 其他组件得分过高，掩盖重复惩罚
- ✗ 重复检测机制在浮点数精度上可能有问题

### 2. 多样性权重动态降低适得其反

**设计**: `gamma_diversity` 从0.8线性降至0.2

**问题**:

- 前期随机探索已经提供了多样性
- 后期多样性权重降低导致过度exploitation
- 与重复惩罚不足叠加，导致严重重复

### 3. 分区均匀性导致局部陷阱

**设计**: `gamma_binning=0.4` 鼓励采样稀疏分区

**问题**:

- 某些低分区间被过度采样
- `orange | grid | Font=12 | none` (True=7.72) 在中低分区，被反复选择
- 分区管理与exploration不平衡

### 4. 信息增益计算可能有偏

**设计**: 动态调整交互效应权重

**问题**:

- GP方差计算可能对某些设计高估信息增益
- 与其他组件竞争时，可能导致不平衡

### 5. 候选集生成策略不当

**当前**: 使用`samps=1000`随机网格采样

**问题**:

- 随机采样可能导致已采样设计在候选集中比例高
- 没有主动排除已采样设计

## 经验教训

### ❌ 失败的技术路线

1. **过度复杂的多组件权重平衡**
   - 4个组件(信息增益、多样性、分区、exploration)权重难以调优
   - 组件间相互竞争，导致不可预测的行为

2. **弱重复惩罚 + 强分区管理**
   - 分区管理驱动重复采样低分区
   - 重复惩罚不足以抵消

3. **动态降低多样性权重**
   - 在后期过度exploitation
   - 应该保持一定的多样性下限

### ✅ V1成功的原因

V1虽然简单，但表现更好：

1. **简单的两组件设计**
   - 信息增益 + 覆盖度
   - 权重固定且平衡 (gamma=0.65)

2. **适度的动态权重**
   - 只调整交互效应权重
   - 不过度干预

3. **没有分区管理**
   - 避免了局部陷阱
   - 自然平衡

## 改进建议 (V3方向)

基于失败经验，真正有效的改进应该是：

### 1. 强化重复惩罚

```python
# 方案A: 直接排除
if design_key in sampled_designs:
    score = -inf  # 完全排除

# 方案B: 指数惩罚
penalty = 0.0001 ** n_repeats  # 重复n次后，得分降至0.0001^n
```

### 2. 候选集预过滤

```python
# 生成候选集时主动排除已采样设计
candidates = [x for x in all_candidates if x not in sampled_designs]
```

### 3. 简化组件结构

**保留**:

- 信息增益 (主效应 + 交互效应)
- 覆盖度 (Gower距离)

**移除**:

- 分区均匀性 (容易导致局部陷阱)
- 单独的exploration项 (GP不确定性已包含在信息增益中)

### 4. 保持多样性权重稳定

```python
# 不要动态降低
gamma_diversity = 0.5  # 固定
```

### 5. 更激进的exploration-exploitation平衡

```python
# 前20%: 高exploration (lambda_inter=0.5)
# 中60%: 平衡 (lambda_inter=1.0→2.0线性)
# 后20%: 高exploitation (lambda_inter=2.5)
```

## 总结

本次改进实验**失败的核心原因**是：

1. **重复惩罚机制失效** - 设计过弱，执行不当
2. **多组件权重难以平衡** - 复杂度过高，适得其反
3. **分区管理引入局部陷阱** - 过度优化某些低分区

**关键启示**:

- ✓ 简单有效胜过复杂精巧
- ✓ V1的两组件设计已经相当合理
- ✓ 重复采样问题需要用"硬约束"而非"软惩罚"
- ✓ 不要为了改进而改进，先诊断真正的瓶颈

## 下一步行动

如果要继续改进，建议：

1. **保留V1框架** - 两组件设计(信息增益+覆盖度)
2. **只加强重复控制** - 直接排除已采样设计
3. **调整候选集生成** - 预过滤 + 增加未采样设计权重
4. **小幅度调参** - 不做结构性变更

**预期改进空间**:

- 唯一设计数: 39 → 50-60 (目标15-17%)
- 高分发现: 8 → 12-15 (目标提升50%+)
- 避免重复: 完全消除相同设计的重复采样

---

**教训**: 复杂的多目标优化往往不如简单而有效的单点改进。
