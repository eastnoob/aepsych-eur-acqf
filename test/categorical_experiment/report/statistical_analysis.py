"""
æ·±åº¦ç»Ÿè®¡åˆ†æï¼šé‡‡æ ·è´¨é‡è¯„ä¼°
åˆ†æé‡‡æ ·æ•°æ®ç›¸è¾ƒäºçœŸå®æ¨¡å‹çš„è¯¯å·®å’Œä»£è¡¨æ€§
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import kstest, anderson, shapiro, mannwhitneyu
import json
from pathlib import Path
import sys

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥è™šæ‹Ÿç”¨æˆ·
sys.path.insert(0, str(Path(__file__).parent.parent))
from virtual_user import VirtualUser

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams["font.sans-serif"] = ["Microsoft YaHei", "SimHei", "Arial"]
plt.rcParams["axes.unicode_minus"] = False
plt.style.use("seaborn-v0_8-whitegrid")

# åŠ è½½æ•°æ®
data_dir = Path(__file__).parent.parent / "results"
trial_data = pd.read_csv(data_dir / "trial_data_20251030_000437.csv")
with open(data_dir / "metadata_20251030_000437.json", "r") as f:
    metadata = json.load(f)

print("=" * 80)
print("æ·±åº¦ç»Ÿè®¡åˆ†æï¼šé‡‡æ ·è´¨é‡è¯„ä¼°")
print("=" * 80)
print(f"æ•°æ®é›†: 80ä¸ªé‡‡æ ·ç‚¹ / 360ä¸ªæ€»è®¾è®¡ç»„åˆ (22.2%)")
print()

# ==================== 1. ç”Ÿæˆå®Œæ•´è®¾è®¡ç©ºé—´çš„çœŸå®åˆ†æ•° ====================
print("1. ç”Ÿæˆå®Œæ•´è®¾è®¡ç©ºé—´ (360ä¸ªç»„åˆ)...")

# åˆ›å»ºè™šæ‹Ÿç”¨æˆ·ï¼ˆä¸å®éªŒç›¸åŒçš„é…ç½®ï¼‰
user = VirtualUser(user_type="balanced", noise_level=0.0, seed=42)  # æ— å™ªå£°çš„çœŸå®åˆ†æ•°

# ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆ
color_schemes = ["blue", "green", "red", "purple", "orange"]
layouts = ["grid", "list", "card", "timeline"]
font_sizes = [12, 14, 16, 18, 20, 22]
animations = ["none", "subtle", "dynamic"]

all_designs = []
for color in color_schemes:
    for layout in layouts:
        for font in font_sizes:
            for anim in animations:
                design = {
                    "color_scheme": color,
                    "layout": layout,
                    "font_size": font,
                    "animation": anim,
                }
                # è®¡ç®—çœŸå®åˆ†æ•°ï¼ˆæ— å™ªå£°ï¼‰
                true_score = user.get_ground_truth(design)
                all_designs.append({**design, "true_score": true_score})

full_space_df = pd.DataFrame(all_designs)
print(f"âœ“ ç”Ÿæˆ {len(full_space_df)} ä¸ªè®¾è®¡çš„çœŸå®åˆ†æ•°")
print()

# ==================== 2. é‡‡æ ·è¦†ç›–åˆ†æ ====================
print("=" * 80)
print("2. é‡‡æ ·è¦†ç›–åˆ†æ")
print("=" * 80)

# æ ‡è®°å“ªäº›è®¾è®¡è¢«é‡‡æ ·
sampled_designs = trial_data[
    ["color_scheme", "layout", "font_size", "animation"]
].copy()
full_space_df["sampled"] = False

for idx, row in sampled_designs.iterrows():
    mask = (
        (full_space_df["color_scheme"] == row["color_scheme"])
        & (full_space_df["layout"] == row["layout"])
        & (full_space_df["font_size"] == row["font_size"])
        & (full_space_df["animation"] == row["animation"])
    )
    full_space_df.loc[mask, "sampled"] = True

n_sampled = full_space_df["sampled"].sum()
print(f"å”¯ä¸€è®¾è®¡é‡‡æ ·: {n_sampled}/360 ({n_sampled/360*100:.1f}%)")
print()

# åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”
sampled_scores = full_space_df[full_space_df["sampled"]]["true_score"]
unsampled_scores = full_space_df[~full_space_df["sampled"]]["true_score"]

print("åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡:")
print(
    f"  å…¨ç©ºé—´: Î¼={full_space_df['true_score'].mean():.3f}, Ïƒ={full_space_df['true_score'].std():.3f}"
)
print(f"  å·²é‡‡æ ·: Î¼={sampled_scores.mean():.3f}, Ïƒ={sampled_scores.std():.3f}")
print(f"  æœªé‡‡æ ·: Î¼={unsampled_scores.mean():.3f}, Ïƒ={unsampled_scores.std():.3f}")
print()

# ==================== 3. åˆ†å¸ƒä¸€è‡´æ€§æ£€éªŒ ====================
print("=" * 80)
print("3. åˆ†å¸ƒä¸€è‡´æ€§æ£€éªŒ")
print("=" * 80)

# 3.1 Kolmogorov-Smirnovæ£€éªŒ
ks_stat, ks_pvalue = kstest(
    sampled_scores,
    lambda x: stats.percentileofscore(full_space_df["true_score"], x) / 100,
)
print("3.1 Kolmogorov-Smirnovæ£€éªŒ (é‡‡æ · vs å…¨ç©ºé—´)")
print(f"  KSç»Ÿè®¡é‡: {ks_stat:.4f}")
print(f"  p-value: {ks_pvalue:.4f}")
print(f"  ç»“è®º: {'åˆ†å¸ƒç›¸ä¼¼ (p>0.05)' if ks_pvalue > 0.05 else 'åˆ†å¸ƒå·®å¼‚æ˜¾è‘— (p<0.05)'}")
print()

# 3.2 Mann-Whitney Uæ£€éªŒ (éå‚æ•°æ£€éªŒ)
u_stat, u_pvalue = mannwhitneyu(
    sampled_scores, unsampled_scores, alternative="two-sided"
)
print("3.2 Mann-Whitney Uæ£€éªŒ (é‡‡æ · vs æœªé‡‡æ ·)")
print(f"  Uç»Ÿè®¡é‡: {u_stat:.2f}")
print(f"  p-value: {u_pvalue:.4f}")
print(
    f"  ç»“è®º: {'ä¸­ä½æ•°æ— æ˜¾è‘—å·®å¼‚ (p>0.05)' if u_pvalue > 0.05 else 'ä¸­ä½æ•°æœ‰æ˜¾è‘—å·®å¼‚ (p<0.05)'}"
)
print()

# 3.3 åˆ†ä½æ•°å¯¹æ¯”
print("3.3 åˆ†ä½æ•°å¯¹æ¯”")
quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]
print("  åˆ†ä½æ•°    å…¨ç©ºé—´    å·²é‡‡æ ·    å·®å¼‚")
print("  " + "-" * 40)
for q in quantiles:
    q_full = full_space_df["true_score"].quantile(q)
    q_sampled = sampled_scores.quantile(q)
    diff = q_sampled - q_full
    print(f"  {q:4.0%}      {q_full:6.3f}    {q_sampled:6.3f}    {diff:+.3f}")
print()

# ==================== 4. é¢„æµ‹è¯¯å·®åˆ†æ ====================
print("=" * 80)
print("4. é¢„æµ‹è¯¯å·®åˆ†æ (è§‚æµ‹è¯„åˆ† vs çœŸå®åˆ†æ•°)")
print("=" * 80)

# åˆå¹¶æ•°æ®
trial_data_with_true = trial_data.merge(
    full_space_df[["color_scheme", "layout", "font_size", "animation", "true_score"]],
    on=["color_scheme", "layout", "font_size", "animation"],
    how="left",
    suffixes=("_observed", "_true"),
)

# æ³¨æ„ï¼štrial_dataå·²ç»æœ‰true_scoreåˆ—äº†
errors = trial_data["rating"] - trial_data["true_score"]

print(f"è¯¯å·®ç»Ÿè®¡ (è§‚æµ‹è¯„åˆ† - çœŸå®åˆ†æ•°):")
print(f"  å¹³å‡è¯¯å·® (ME):  {errors.mean():.3f}")
print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE): {np.abs(errors).mean():.3f}")
print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE): {np.sqrt((errors**2).mean()):.3f}")
print(f"  æ ‡å‡†å·®: {errors.std():.3f}")
print(f"  ä¸­ä½æ•°è¯¯å·®: {errors.median():.3f}")
print()

# è¯¯å·®åˆ†å¸ƒ
print("è¯¯å·®åˆ†å¸ƒ:")
for threshold in [0.5, 1.0, 1.5, 2.0]:
    pct = (np.abs(errors) <= threshold).mean() * 100
    print(f"  Â±{threshold}ä»¥å†…: {pct:.1f}%")
print()

# ==================== 5. é‡‡æ ·åå·®åˆ†æ ====================
print("=" * 80)
print("5. é‡‡æ ·åå·®åˆ†æ")
print("=" * 80)

# 5.1 åˆ†æ•°åŒºé—´è¦†ç›–
score_bins = [6, 7, 8, 9, 10, 11]
full_space_df["score_bin"] = pd.cut(full_space_df["true_score"], bins=score_bins)

print("5.1 åˆ†æ•°åŒºé—´è¦†ç›–")
print("  åŒºé—´        å…¨ç©ºé—´   å·²é‡‡æ ·   é‡‡æ ·ç‡")
print("  " + "-" * 45)
for bin_range in full_space_df["score_bin"].cat.categories:
    n_full = (full_space_df["score_bin"] == bin_range).sum()
    n_sampled = (
        full_space_df[full_space_df["sampled"]]["score_bin"] == bin_range
    ).sum()
    rate = n_sampled / n_full * 100 if n_full > 0 else 0
    print(f"  {bin_range}    {n_full:4d}    {n_sampled:4d}    {rate:5.1f}%")
print()

# 5.2 é«˜åˆ†è®¾è®¡å‘ç°ç‡
top_percentiles = [90, 95, 99]
print("5.2 é«˜åˆ†è®¾è®¡å‘ç°ç‡")
print("  ç™¾åˆ†ä½    é˜ˆå€¼    å…¨ç©ºé—´   å·²é‡‡æ ·   å‘ç°ç‡")
print("  " + "-" * 50)
for pct in top_percentiles:
    threshold = full_space_df["true_score"].quantile(pct / 100)
    n_full = (full_space_df["true_score"] >= threshold).sum()
    n_sampled = (
        full_space_df[full_space_df["sampled"]]["true_score"] >= threshold
    ).sum()
    rate = n_sampled / n_full * 100 if n_full > 0 else 0
    print(
        f"  Top {100-pct}%    {threshold:5.2f}    {n_full:4d}    {n_sampled:4d}    {rate:5.1f}%"
    )
print()

# ==================== 6. å˜é‡æ•ˆåº”åˆ†æ ====================
print("=" * 80)
print("6. å˜é‡ä¸»æ•ˆåº”åˆ†æ")
print("=" * 80)

variables = ["color_scheme", "layout", "font_size", "animation"]

for var in variables:
    print(f"\n{var}:")
    # å…¨ç©ºé—´
    full_means = full_space_df.groupby(var)["true_score"].agg(["mean", "count"])
    # é‡‡æ ·ç©ºé—´
    sampled_means = (
        full_space_df[full_space_df["sampled"]]
        .groupby(var)["true_score"]
        .agg(["mean", "count"])
    )

    print(f"  æ°´å¹³       å…¨ç©ºé—´å‡å€¼  é‡‡æ ·å‡å€¼  æ ·æœ¬æ•°  å·®å¼‚")
    print("  " + "-" * 55)
    for level in full_means.index:
        full_mean = full_means.loc[level, "mean"]
        if level in sampled_means.index:
            samp_mean = sampled_means.loc[level, "mean"]
            samp_count = sampled_means.loc[level, "count"]
            diff = samp_mean - full_mean
            print(
                f"  {str(level):12s} {full_mean:8.3f}    {samp_mean:7.3f}    {samp_count:3.0f}    {diff:+.3f}"
            )
        else:
            print(f"  {str(level):12s} {full_mean:8.3f}    --        0     --")

print()

# ==================== 7. ä¿¡æ¯é‡åˆ†æ ====================
print("=" * 80)
print("7. ä¿¡æ¯é‡åˆ†æ")
print("=" * 80)

# 7.1 é‡‡æ ·çš„ä¿¡æ¯ç†µ
from scipy.stats import entropy


def compute_entropy(scores):
    """è®¡ç®—åˆ†æ•°åˆ†å¸ƒçš„ç†µ"""
    hist, _ = np.histogram(scores, bins=20, density=True)
    hist = hist[hist > 0]  # ç§»é™¤0å€¼
    return entropy(hist)


entropy_full = compute_entropy(full_space_df["true_score"])
entropy_sampled = compute_entropy(sampled_scores)

print(f"Shannonç†µ:")
print(f"  å…¨ç©ºé—´: {entropy_full:.4f}")
print(f"  å·²é‡‡æ ·: {entropy_sampled:.4f}")
print(f"  ç†µæ¯”ç‡: {entropy_sampled/entropy_full:.2%}")
print()

# 7.2 ä»£è¡¨æ€§æŒ‡æ ‡
# ä½¿ç”¨å˜å¼‚ç³»æ•° (CV = Ïƒ/Î¼)
cv_full = full_space_df["true_score"].std() / full_space_df["true_score"].mean()
cv_sampled = sampled_scores.std() / sampled_scores.mean()

print(f"å˜å¼‚ç³»æ•° (CV = Ïƒ/Î¼):")
print(f"  å…¨ç©ºé—´: {cv_full:.4f}")
print(f"  å·²é‡‡æ ·: {cv_sampled:.4f}")
print(f"  å·®å¼‚: {abs(cv_sampled - cv_full)/cv_full*100:.1f}%")
print()

# ==================== 8. ç»¼åˆè¯„ä¼° ====================
print("=" * 80)
print("8. ç»¼åˆè¯„ä¼°")
print("=" * 80)

# è®¡ç®—ç»¼åˆæŒ‡æ ‡
coverage_score = n_sampled / 360  # è¦†ç›–ç‡
distribution_score = 1 - abs(ks_stat)  # KSç»Ÿè®¡é‡è¶Šå°è¶Šå¥½
error_score = 1 - (metadata["metrics"]["mae"] / 10)  # MAEå½’ä¸€åŒ–
correlation_score = metadata["metrics"]["correlation"]  # ç›¸å…³ç³»æ•°

# é«˜åˆ†å‘ç°
top_10pct_threshold = full_space_df["true_score"].quantile(0.9)
top_10pct_found = (
    full_space_df[full_space_df["sampled"]]["true_score"] >= top_10pct_threshold
).sum()
discovery_score = top_10pct_found / (
    len(full_space_df) * 0.1
)  # å‘ç°äº†å¤šå°‘æ¯”ä¾‹çš„top 10%

print("ç»¼åˆæŒ‡æ ‡å¾—åˆ† (0-1):")
print(f"  è¦†ç›–ç‡:        {coverage_score:.3f}  {'â­'*int(coverage_score*5)}")
print(f"  åˆ†å¸ƒä¸€è‡´æ€§:    {distribution_score:.3f}  {'â­'*int(distribution_score*5)}")
print(f"  é¢„æµ‹å‡†ç¡®æ€§:    {error_score:.3f}  {'â­'*int(error_score*5)}")
print(f"  ç›¸å…³æ€§:        {correlation_score:.3f}  {'â­'*int(correlation_score*5)}")
print(f"  é«˜åˆ†å‘ç°:      {discovery_score:.3f}  {'â­'*int(discovery_score*5)}")
print()

overall_score = (
    coverage_score
    + distribution_score
    + error_score
    + correlation_score
    + discovery_score
) / 5
print(f"æ€»ä½“è¯„åˆ†: {overall_score:.3f} / 1.000  {'â­'*int(overall_score*5)}")
print()

# ==================== 9. ç»“è®º ====================
print("=" * 80)
print("9. ç»“è®ºä¸å»ºè®®")
print("=" * 80)

conclusions = []

# è¦†ç›–æ€§
if n_sampled / 360 > 0.15:
    conclusions.append("âœ“ è¦†ç›–æ€§: è‰¯å¥½ - é‡‡æ ·äº†10.8%çš„è®¾è®¡ç©ºé—´")
else:
    conclusions.append("âœ— è¦†ç›–æ€§: ä¸è¶³ - éœ€è¦å¢åŠ é‡‡æ ·æ•°é‡")

# åˆ†å¸ƒä¸€è‡´æ€§
if ks_pvalue > 0.05:
    conclusions.append(
        f"âœ“ åˆ†å¸ƒä¸€è‡´æ€§: ä¼˜ç§€ - é‡‡æ ·åˆ†å¸ƒä¸å…¨ç©ºé—´æ— æ˜¾è‘—å·®å¼‚ (p={ks_pvalue:.3f})"
    )
else:
    conclusions.append(
        f"âš  åˆ†å¸ƒä¸€è‡´æ€§: æœ‰åå·® - é‡‡æ ·åˆ†å¸ƒä¸å…¨ç©ºé—´æœ‰å·®å¼‚ (p={ks_pvalue:.3f})"
    )

# é¢„æµ‹å‡†ç¡®æ€§
if metadata["metrics"]["within_1"] >= 0.8:
    conclusions.append(
        f"âœ“ é¢„æµ‹å‡†ç¡®æ€§: ä¼˜ç§€ - {metadata['metrics']['within_1']*100:.0f}%çš„é¢„æµ‹åœ¨Â±1ä»¥å†…"
    )
else:
    conclusions.append(
        f"âš  é¢„æµ‹å‡†ç¡®æ€§: ä¸€èˆ¬ - ä»…{metadata['metrics']['within_1']*100:.0f}%çš„é¢„æµ‹åœ¨Â±1ä»¥å†…"
    )

# é«˜åˆ†å‘ç°
if discovery_score > 0.5:
    conclusions.append(
        f"âœ“ é«˜åˆ†å‘ç°: ä¼˜ç§€ - å‘ç°äº†{discovery_score*100:.0f}%çš„top 10%è®¾è®¡"
    )
else:
    conclusions.append(
        f"âš  é«˜åˆ†å‘ç°: ä¸è¶³ - ä»…å‘ç°äº†{discovery_score*100:.0f}%çš„top 10%è®¾è®¡"
    )

# å˜é‡è¦†ç›–
all_vars_covered = all(
    [
        metadata["coverage"][var]["coverage"] >= 1.0
        for var in ["color_scheme", "layout", "animation"]
    ]
)
if all_vars_covered:
    conclusions.append("âœ“ å˜é‡è¦†ç›–: å®Œæ•´ - æ‰€æœ‰å˜é‡çš„æ‰€æœ‰æ°´å¹³éƒ½è¢«é‡‡æ ·")
else:
    conclusions.append("âš  å˜é‡è¦†ç›–: ä¸å®Œæ•´ - éƒ¨åˆ†å˜é‡æ°´å¹³æœªè¢«é‡‡æ ·")

for conclusion in conclusions:
    print(conclusion)

print()
print("æ€»ç»“:")
if overall_score >= 0.7:
    print("ğŸ‰ é‡‡æ ·è´¨é‡ä¼˜ç§€ï¼é‡‡æ ·æ•°æ®èƒ½å¤Ÿè¾ƒå¥½åœ°ä»£è¡¨å®Œæ•´è®¾è®¡ç©ºé—´ã€‚")
    print("   å¯ä»¥åŸºäºè¿™äº›æ•°æ®è¿›è¡Œå¯é çš„ç»Ÿè®¡æ¨æ–­å’Œå»ºæ¨¡ã€‚")
elif overall_score >= 0.5:
    print("âœ“ é‡‡æ ·è´¨é‡è‰¯å¥½ã€‚é‡‡æ ·æ•°æ®åŸºæœ¬èƒ½å¤Ÿä»£è¡¨è®¾è®¡ç©ºé—´çš„ä¸»è¦ç‰¹å¾ã€‚")
    print("  å»ºè®®åœ¨å…³é”®åŒºåŸŸï¼ˆå¦‚é«˜åˆ†åŒºï¼‰å¢åŠ é‡‡æ ·ä»¥æé«˜ä»£è¡¨æ€§ã€‚")
else:
    print("âš  é‡‡æ ·è´¨é‡ä¸€èˆ¬ã€‚é‡‡æ ·æ•°æ®å¯èƒ½æ— æ³•å®Œå…¨ä»£è¡¨è®¾è®¡ç©ºé—´ã€‚")
    print("  å»ºè®®å¢åŠ é‡‡æ ·æ•°é‡æˆ–æ”¹è¿›é‡‡æ ·ç­–ç•¥ã€‚")

print("\n" + "=" * 80)
print("åˆ†æå®Œæˆï¼è¯¦ç»†å¯è§†åŒ–æŠ¥å‘Šå°†ä¿å­˜åˆ° report/ ç›®å½•ã€‚")
print("=" * 80)

# ä¿å­˜è¯¦ç»†æ•°æ®
output_dir = Path(__file__).parent
full_space_df.to_csv(output_dir / "full_design_space_analysis.csv", index=False)
print(f"\nâœ“ å®Œæ•´è®¾è®¡ç©ºé—´æ•°æ®å·²ä¿å­˜: full_design_space_analysis.csv")

# ä¿å­˜ç»Ÿè®¡ç»“æœ
stats_results = {
    "coverage": {
        "sampled_designs": int(n_sampled),
        "total_designs": 360,
        "coverage_rate": float(n_sampled / 360),
    },
    "distribution": {
        "ks_statistic": float(ks_stat),
        "ks_pvalue": float(ks_pvalue),
        "mann_whitney_u": float(u_stat),
        "mann_whitney_pvalue": float(u_pvalue),
    },
    "errors": {
        "mean_error": float(errors.mean()),
        "mae": float(np.abs(errors).mean()),
        "rmse": float(np.sqrt((errors**2).mean())),
        "std": float(errors.std()),
    },
    "information": {
        "entropy_full": float(entropy_full),
        "entropy_sampled": float(entropy_sampled),
        "cv_full": float(cv_full),
        "cv_sampled": float(cv_sampled),
    },
    "scores": {
        "coverage_score": float(coverage_score),
        "distribution_score": float(distribution_score),
        "error_score": float(error_score),
        "correlation_score": float(correlation_score),
        "discovery_score": float(discovery_score),
        "overall_score": float(overall_score),
    },
}

with open(output_dir / "statistical_results.json", "w") as f:
    json.dump(stats_results, f, indent=2)
print(f"âœ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜: statistical_results.json")
